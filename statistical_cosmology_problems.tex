\documentclass[aps,prd,superscriptaddress,groupedaddress,nofootinbib,nobibnotes]{revtex4}

\usepackage{graphicx}
\usepackage{dcolumn}
\usepackage{bm}
\usepackage{amssymb}
\usepackage{epstopdf}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{color}
\usepackage{mathrsfs}
% \usepackage{comment}
% \usepackage{url}
% \usepackage{wick}
% \usepackage{feynmp}
% \usepackage{braket}

\setlength{\parindent}{20pt}
% \setlength{\parskip}{1mm}

\setcounter{topnumber}{1}    % default value is 2.
\setcounter{bottomnumber}{0} % default value is 1.

\hyphenation{ALPGEN}
\hyphenation{EVTGEN}
\hyphenation{PYTHIA}

\newcommand{\kms}[1]{\textcolor{blue}{(KMS: #1)}}
\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
\newcommand{\ba}{\begin{eqnarray}}
\newcommand{\ea}{\end{eqnarray}}
\newcommand{\nn}{\nonumber}
\newcommand{\barr}{\begin{array}}
\newcommand{\earr}{\end{array}}
\newcommand{\eqdef}{\stackrel{\rm def}{=}}
\newcommand{\bigoh}{\mathcal{O}}

\newcommand\lsim{\mathrel{\rlap{\lower4pt\hbox{\hskip1pt$\sim$}}
        \raise1pt\hbox{$<$}}}
\newcommand\gsim{\mathrel{\rlap{\lower4pt\hbox{\hskip1pt$\sim$}}
        \raise1pt\hbox{$>$}}}

\def\threej#1#2#3#4#5#6{\left( \begin{array}{ccc} #1 & #2 & #3 \\ #4 & #5 & #6 \end{array} \right) }
\def\smallsum{\mathop{\textstyle\sum}\limits}
\def\Var{\mbox{Var}}
\def\Cov{\mbox{Cov}}
\def\x{{\bf x}}
\def\y{{\bf y}}
\def\r{{\bf r}}
\def\k{{\bf k}}
\def\l{{\bf l}}
\def\L{{\mathcal L}}
\def\hpi{{\hat\pi}}
\def\n{{\bf n}}
\def\tphi{{\tilde\phi}}

\renewcommand{\baselinestretch}{1.1}

\begin{document}

\title{Statistical cosmology problems}

\author{Kendrick~M.~Smith}
\affiliation{Perimeter Institute for Theoretical Physics, Waterloo, ON N2L 2Y5, Canada}

\date{\today}

% \begin{abstract}
% ABSTRACT HERE
% \end{abstract}
% \pacs{}

\maketitle
% \tableofcontents

\section{Power spectra}

\par\noindent
In this section, let $\phi(\x)$ be a 3D random field with power spectrum $P(k)$ defined by
\be
\langle \phi_{\k}^* \phi_{\k'} \rangle = P(k) \, (2\pi)^3 \delta^3(\k-\k')  \label{eq:pk_def}
\ee

\begin{enumerate}

\item What symmetry assumptions are made in writing down Eq.~(\ref{eq:pk_def})?
 Show how Eq.~(\ref{eq:pk_def}) follows from these assumptions.

\item Suppose we define a new random field $\psi$ by $\psi(\x) = \nabla^2 \phi(\x)$.
What is the power spectrum $P_\psi(k)$ of the $\psi$ field (in terms of the original power spectrum $P(k)$)?

\item {\bf Scaling.}
Suppose that we define the field $\psi(\x) = \phi(\lambda\x)$ by rescaling coordinates,
where $\lambda$ is a constant.  Show that the power spectrum scales as
\be
P_\psi(k) = \lambda^{-3} P(\lambda^{-1} k)
\ee
Note that as a consequence, if a field's power spectrum is proportional to $k^{-3}$, 
then it is {\em scale-invariant}, in the sense that magnifying/demagnifying does 
not change its statistical properties.

\item {\bf Slicing.}
Suppose we define a {\em two-dimensional} random field $\psi(\x)$ by simply restricting 
the 3D field $\phi$ to a 2D plane, i.e. in real space:
\be
\psi(x,y) = \phi(x,y,0)
\ee
Let $P_\psi(l)$ be the power spectrum of this field.  (Note that we use $l$ for Fourier wavenumbers in 2D, and
$k$ in 3D.)  Show that $P_\psi(l)$ is related to the original 3D power spectrum $P(k)$ by
\be
P_\psi(l) = \int_l^\infty dk \, \frac{k}{2\pi\sqrt{k^2-l^2}} P(k)
\ee
Suppose the 3D power spectrum has the scale-invariant form $P(k) = A k^{-3}$.
What is $P_\psi(l)$ in this case?  Is it also scale invariant?

\item {\bf Slicing, part 2.}
In the previous problem, we worked out the transformation law for the power spectrum,
when a 3D field is restricted on a 2D plane.  What is the analogous transformation law
when restricting to a 1D line?

\item {\bf Convolution.}
Suppose we define a new field $\psi(\x)$ by convolving with a kernel in real space.
\be
\psi(\x) = \int d^3\y \, W(|\x-\y|) \phi(\y)
\ee
where $W(r)$ is a kernel which we assume only depends on $r = |\r|$.  How are the Fourier transforms $\tilde\psi(\k)$
and $\tilde\phi(\k)$ related?  How is the power spectrum $P_\psi(k)$ related to $P(k)$?

\item {\bf Tophat averaging.}
Suppose we define a new field $\psi(\x)$ by ``tophat averaging'' the field $\phi$.
That is, the value of $\psi$ at any point $\x$ is given by averaging $\phi$ over a ball of radius $R$ centered at $\x$.
How is the power spectrum $P_\psi(k)$ related to $P(k)$?  (Hint: use the result of the previous problem.)

\end{enumerate}

\section{Power spectrum forecasts}

\par\noindent
In the first two problems, the setup is as follows.
We are given as ``data'' $N$ independent samples $x_1, \cdots, x_N$ from a Gaussian distribution.
Suppose that the mean of the distribution is known in advance to be zero, but the variance $\eta$ is not known in advance.
We are interested in forecasting the statistical error $\sigma(\eta)$ on the parameter $\eta$, when it is estimated from the data $x_i$.
This toy example is a warmup for the power spectrum forecasts which follow.

\begin{enumerate}

\item {\bf Statistical error on variance of a Gaussian, frequentist version.} 
 In frequentist statistics, we construct an estimator $\hat\eta$ for the parameter $\eta$, given the data $x_i$.
 In general, the question of which estimator to use can be nontrivial and require calculation, but in this
 simple example there is only one natural choice:
\be
 \hat\eta = \frac{1}{N} \sum_{i=1}^N x_i^2
\ee
 Show that $\hat\eta$ is an unbiased estimator of $\eta$ (i.e.~$\langle \hat\eta \rangle = \eta$).
 By computing the variance $\mbox{Var}(\hat\eta)$, determine the statistical error $\sigma(\eta)$.
 You may find formulas in Appendix~\ref{app:gaussian} useful.

\item {\bf Statistical error on variance of a Gaussian, Bayesian version.}
 In Bayesian statistics, we make inferences on $\eta$ given the data $x_i$ by
 using the posterior likelihood $\L[\eta|x_i]$.
 
 Assuming a flat prior on $\eta$, write an expression for the posterior likelihood $\L[\eta|x_i]$.
 At what value of $\eta$ is the likelihood maximized?  (The result will depend on the data $x_i$.)

 A standard Bayesian method for forecasting the statistical error $\sigma(\eta)$
 is to compute the Fisher matrix, which we briefly review here.
 Here, the Fisher ``matrix'' is a 1-by-1 matrix $F$, since there is only one parameter:
\be
 F = -\left\langle \frac{\partial^2 \log\L(\eta|x_i)}{\partial \eta^2} \right\rangle  \label{eq:F_eta}
\ee
 where the expectation value $\langle \cdot \rangle$ is taken over random realizations of the data $x_i$
 for a fixed value of the parameter $\eta$.  Note that the posterior likelihood $\L(\eta|x_i)$ depends on
 both $\eta$ and $x_i$, but the Fisher matrix only depends on $\eta$.  The forecasted error $\sigma(\eta)$
 is related to the Fisher matrix by $\sigma(\eta) = F^{-1/2}$.

 Compute the Fisher matrix using Eq.~(\ref{eq:F_eta}) and the forecasted statistical error $\sigma(\eta)$.
 Does the result agree with your frequentist forecast in the previous question?

\newcounter{enumi_save}
\setcounter{enumi_save}{\value{enumi}}
\end{enumerate}

\medskip\par\noindent
The setup for the remaining problems will be as follows.
Let $\phi_{\k}$ be a 3D Gaussian random field with power spectrum $P(k)$.
Suppose that $\phi$ is defined in a box with volume $V$ and periodic boundary conditions, so that the Fourier wavenumbers $\k$ form a discrete set.
We assume that field $\phi$ has been observed over some range of scales, so that our ``data'' consists of
complex numbers $\phi_{\k_1}, \phi_{\k_2}, \cdots$ for some finite set of wavenumbers $\k$.

Suppose that the power spectrum $P(k)$ depends on parameters $\pi_1, \cdots, \pi_N$ which are not known in advance, 
but must be inferred from the data.
When we want to denote this dependence explicitly, we will write the power spectrum as $P(k,\pi_i)$,
a function of $(N+1)$ variables.

For example, the parameters $\pi_i$ might be cosmological parameters $(\Omega_b, \Omega_m, \cdots)$,
and $\phi$ might be the cosmological density field, whose power spectrum depends on cosmological parameters
in a complicated way, which is computable using software such as CAMB or CLASS.
We will assume that we have rough fiducial guesses for the parameters $\pi_i^{\rm fid}$, and therefore a rough
fiducial guess for the power spectrum $P_{\rm fid}(k) = P(k,\pi_i^{\rm fid})$.

We are interested in forecasting statistical errors $\sigma(\pi_i)$ on the parameters $\pi_i$
when the modes of the field $\phi_{\k}$ are measured.
Note that when forecasting the error $\sigma(\pi_i)$, we can choose to either {\em marginalize}
the remaining parameters $\pi_{j\ne i}$, or {\em fix} these parameters to their fiducial values.
By default, if we write $\sigma(\pi_i)$ without indicating this choice explicitly, then the
remaining parameters $\pi_{j\ne i}$ are assumed marginalized.

\begin{enumerate}
\setcounter{enumi}{\value{enumi_save}}

\item {\bf Statistical error on power spectrum, Bayesian version.}
  In Bayesian statistics, we can forecast statistical errors on parameters using the Fisher matrix,
  which we briefly review here.  In this context, the Fisher matrix is defined by:
\be
F_{ij} = -\left\langle \frac{\partial^2\L[\pi_i|\phi_{\k}]}{\partial\pi_i \partial\pi_j} \right\rangle  \label{eq:F_P}
\ee
  where the expectation value $\langle \cdot \rangle$ is taken over random realizations of the field $\phi$,
  for fixed values of the parameters $\pi_i$.
  The statistical error on parameter $i$ is given by either $(F_{ii})^{-1/2}$ or $(F^{-1}_{ii})^{1/2}$,
  depending on whether the remaining parameters $\pi_{j\ne i}$ are assumed fixed or marginalized.  In a
  fully marginalized analysis, the covariance matrix of the parameters is the inverse Fisher matrix:
  $\mbox{Cov}(\pi_i, \pi_j) = F^{-1}_{ij}$.

  Write an expression for the posterior likelihood function $\L[\pi_i|\phi_{\k}]$ as a product over observed Fourier modes $\k$.
  Then compute the Fisher matrix using Eq.~(\ref{eq:F_P}), and show that the result can be written as the sum over observed Fourier modes:
\be
F_{ij} = \frac{1}{2} \sum_{\k} \frac{(\partial_i P(k)) (\partial_j P(k))}{P(k)^2}
\ee
\item {\bf Forecasting bandpowers}.
  Suppose that we observe the field $\phi_{\k}$ over some range of scales $k_{\rm min} < k < k_{\rm max}$,
  and that we define this $k$-range into $N$ bands $b_1 = [k_{\rm min}, k_1]$, $b_2 = [k_1, k_2]$, $\cdots$, 
  $b_N = [k_{N-1}, k_{\rm max}]$.  We parametrize the power spectrum by defining ``bandpowers'' $\pi_1, \cdots, \pi_N$:
\be
 P(k) = \left\{
  \begin{array}{cl}
    (1 + \pi_1) P_{\rm fid}(k) & \mbox{ if $k\in b_1$} \\
    (1 + \pi_2) P_{\rm fid}(k) & \mbox{ if $k\in b_2$} \\
      \cdots & \cdots \\
    (1 + \pi_N) P_{\rm fid}(k) & \mbox{ if $k\in b_N$}
  \end{array} \right.
\ee
  Compute the Fisher matrix $F_{ij}$ in this parameter space.
  Assuming that the $k$-sums which arise can be approximated by integrals, what is the simplest way of writing the result?
  What is the statistical error $\sigma(\pi_i)$ on the bandpowers?

\item {\bf Forecasting amplitude and spectral index.}
  Suppose we observe the field $\phi_{\k}$ over some range of scales $k_{\rm min} < k < k_{\rm max}$,
  and that we model the power spectrum as a power law:
\be
  P(k) = P_0 \left( \frac{k}{k_{\rm min}} \right)^\alpha
\ee
  where the parameters are the amplitude $\pi_1 = P_0$ and the spectral index $\pi_2 = \alpha$.
  As usual, we assume that we have rough initial guesses $\pi_i^{\rm fid} = (P_0^{\rm fid}, \alpha_{\rm fid})$
  for these parameters.

  What are the statistical errors on the parameters $P_0$ and $\alpha$, if the complementary parameter
  is assumed fixed?  Assumed marginalized?
  Assume that the $k$-sums which arise can be approximated by integrals.
  The result should be expressible as a fairly simple function of $k_{\rm min}$, $k_{\rm max}$, and the box volume $V$?

\item {\bf Noise and beam.}
  How would the Fisher matrix in Eq.~(\ref{eq:F_P}) change if the observation of the field $\phi_{\k}$ is noisy?
  That is, instead of observing each mode $\phi_\k$ perfectly, suppose we observe the sum $(\phi_\k + \eta_\k)$, where
  the noise $\eta$ is a Gaussian random field with known power spectrum $P_\eta(k)$.

  How would the Fisher matrix in Eq.~(\ref{eq:F_P}) change if the field is convolved with an observational ``beam'', before adding noise?
  That is, assume that we observe the sum $(b_k \phi_\k + \eta_\k)$, where $b_k$ is a known beam profile,
  and $\eta$ is a Gaussian random field with known power spectrum $P_\eta(k)$.

\item 
  {\bf Statistical error on power spectrum, frequentist version.}
  So far, our power spectrum forecasts have used Bayesian statistics, which turns out to be simpler.
  However, Eq.~(\ref{eq:F_P}) for the Fisher matrix can also be derived in a different way, using frequentist statistics.
  Warning: this problem is long and fairly difficult, so I saved it for last!

  First, some setup.
  For each parameter $\pi_i$, we want to construct an estimator $\hpi_i$, whose input is the observed field $\phi_{\k}$,
  and whose output is a real number.  Let us start by writing a general estimator in the form:
\be
  \hpi_i = A_i + \frac{1}{2} \sum_{\k} W_i(k) |\phi_\k|^2  \label{eq:hpi1}
\ee
  with additive offset $A_i$ and $k$-weighting $W_i(k)$ to be determined by a calculation to follow shortly.
  This form of the estimator makes some assumptions, for example we have assumed that the estimator must be quadratic
  in the field $\phi$, which makes intuitive sense because the two-point function of a Gaussian field contains all
  the information.  However we will omit the formal proof that the optimal estimator takes the form in Eq.~(\ref{eq:hpi1}).
  (The factor $1/2$ is a convention which will turn out to be convenient in the calculations below.)

  Leaving $A$ and $W_k$ undetermined for now, calculate the expectation value $\langle \hpi_i \rangle$, assuming that
  the power spectrum is perturbed to {\em first order} in $\delta\pi_i = \pi_i - \pi_i^{\rm fid}$.  That is, assuming:
\be
  P(k) = P_{\rm fid}(k) + \sum_i (\delta \pi_j) \frac{\partial P(k)}{\partial\pi_j}
\ee
  calculate the expectation value of the estimator in Eq.~(\ref{eq:hpi1}), and write the result as the sum of terms
  which are zeroth-order and first-order in $(\delta\pi_j)$.  You result should take the general form:
\be
  \langle \hpi_i \rangle = M_{ij} (\delta \pi_j) + N_i
\ee
  where the matrix $M_{ij}$ and vector $N_i$ will depend on the weights $A$ and $W_i(k)$.

  Next, calculate the variance $\Var(\hpi_i)$ of the estimator in Eq.~(\ref{eq:hpi1}), to zeroth order in $(\delta\pi_i)$.
  That is, assume $P(k) = P_{\rm fid}(k)$.

  We would like to derive the {\em minimum variance unbiased estimator} $\hpi_i$.
  This can be done by minimizing the variance $\Var(\hpi_i)$, subject to the constraint that $\hpi_i$ is an unbiased estimator
  of the parameter $\pi_i$, to first order in $(\delta\pi_j)$.
  That is, we have $\langle \hpi_i \rangle = \pi_i^{\rm fid} + (\delta\pi_i)$, with no contribution from $(\delta\pi_{j \ne i})$.
  This gives the following constraints on $M_{ij}$ and $N_i$:
\be
 M_{ij} = \delta_{ij}
   \hspace{1.5cm}
 N_i = \pi_i^{\rm fid}
\ee
  Using your expressions for $\Var(\hpi_i)$, $M_{ij}$, and $N_i$, solve for the weights $A$ and $W_i(k)$ which
  give the minimum variance quadratic estimator.  (Hint: when solving this constrained optimization problem, you
  may find the method of Lagrange multipliers useful.)

  Finally, now that you have derived the minimum variance quadratic estimator $\hpi_i$,
  calculate the covariance matrix $\mbox{Cov}(\hpi_i, \hpi_j)$.  You should get $F^{-1}_{ij}$,
  where $F_{ij}$ is the Fisher matrix defined previously in Eq.~(\ref{eq:F_P}).  This gives
  an alternate derivation of the Fisher matrix based on frequentist statistics.  

  In this problem, the frequentist version of the calculation is more complicated,
  and it is easiest to just use Bayesian statisitics throughout!  However, there are
  situations in statistical cosmology where the frequentist calculation turns out to be
  easier, so it is useful to work through an example to see how the two formalisms relate.
\end{enumerate}

\clearpage

\section{Curved sky}

So far, we have considered random fields $\phi(\x)$ in either 2D or 3D Euclidean space.
In this section, we will study random fields $\phi(\n)$ defined on the sphere, where $\n = (n_x, n_y, n_z)$
is a unit three-vector.
The canonical example is the cosmic microwave background temperature $T(\n)$.

We will use some special functions in this section: the spherical harmonics $Y_{lm}$,
and the Legendre polynomials $P_l(x)$.  Properties of these special functions are
summarized in Appendix~\ref{app:curved_sky}.

We will omit the proof of the following completeness theorem.
An arbitrary function $\phi(\n)$ may be expanded in spherical harmonics as:
\be
\phi(\n) = \sum_{l=0}^\infty \sum_{m=-l}^l a_{lm} Y_{lm}(\n)   \label{eq:sht1}
\ee
where the coefficients $a_{lm}$ are defined for integers $l,m$ such that $l \ge 0$ and $-l \le m \le l$.
The coefficients $a_{lm}$ can be determined from the field $\phi(\n)$ by doing the following integral:
\be
a_{lm} = \int d^2\n \, \phi(\n) Y_{lm}^*(\n)  \label{eq:sht2}
\ee
This follows from Eq.~(\ref{eq:sht1}), by multiplying both sides by $Y_{lm}(\n)^*$, and using the
orthogonality identity in Eq.~(\ref{eq:ylm_orth}).  Note that $d^2\n$ denotes the rotation-invariant
area element on the sphere (i.e.~$d^2\n = \sin\theta \, d\theta \, d\phi$ in coordinates $\theta,\phi$).

Taken together, Equations~(\ref{eq:sht1}) and~(\ref{eq:sht2}) define an invertible transform 
$\phi(\n) \leftrightarrow a_{lm}$ which exchanges a real-space field $\phi(\n)$ and its harmonic-space
coefficients $a_{lm}$.  This is the ``spherical harmonic transform'', the curved-sky analog of the
2D Euclidean Fourier transform $\phi(\x) \rightarrow \tphi(\l)$, defined by:
\be
\tphi(\l) = \int d^2\x \, \phi(\x) e^{-i\l\cdot\x}
  \hspace{1.5cm}
\phi(\x) = \int \frac{d^2\l}{(2\pi)^2} \tphi(\l) e^{i\l\cdot\x}
\ee
Here is one interesting difference between the spherical harmonic transform and the Fourier transform.
The functional form of the Fourier transform $\phi(\x) \rightarrow \tphi(\l)$ is the same (except for a 
sign change $i \rightarrow -i$) as the inverse Fourier transform $\tphi(\l) \rightarrow \phi(\x)$.
In constrast, the spherical harmonic transform in Eq.~(\ref{eq:sht1}) and its inverse in Eq.~(\ref{eq:sht2})
look very different.
The real-space representation of a curved-sky field (a function $\phi(\n)$) also looks very different from
its harmonic-space representation (a discrete set of coefficients $a_{lm}$).

Here is another theorem whose proof we will omit.
If the statistics of a random field are rotationally invariant, then its harmonic-space two-point
correlation function has the constrained form:
\be
\langle a_{l m}^* a_{l'm'} \rangle C_l \delta_{ll'} \delta_{mm'}   \label{eq:cl_def}
\ee
where the power spectrum $C_l$ is defined by this equation.  This is the curved-sky analog of
Eq.~(\ref{eq:pk_def}), which defines the power spectrum $P(k)$ assuming translation and rotation
invariance.

In the following problems, we will work out more properties of curved-sky fields.

\begin{enumerate}

\item {\bf Real-valuedness.} If $\phi(\n)$ is real-valued in real space, show that the harmonic-space 
 coefficients satisfy $a_{l,-m} = (-1)^m a_{lm}^*$.  Assuming rotation invariance, write an expression 
 for the expectation value
\be
\langle a_{lm} a_{l'm'} \rangle
\ee
 (with no complex conjugate appearing) in terms of the power spectrum $C_l$.  
 What are the analogs of these statements for a 2D Euclidean field $\phi(\x)$?

\item {\bf Variance.} Using identities from Appendix~\ref{app:curved_sky}, show that the variance 
 $\langle \phi(\n)^2 \rangle$ of a curved-sky random field is related to its power spectrum $C_l$ by:
\be
\langle \phi(\n)^2 \rangle = \sum_l \frac{2l+1}{4\pi} C_l
\ee
 What is the Euclidean analog?

\item {\bf Correlation function.} If the statistics of a random field are rotation invariant, then its {\em real-space}
 two-point correlation function $\langle \phi(\n) \phi(\n') \rangle$ only depends on the angle
 $\theta_{\n\n'}$ between $\n$ and $\n'$, or equivalently the dot product $(\n\cdot\n') = \cos(\theta_{\n\n'})$.  
 That is,
\be
 \langle \phi(\n) \phi(\n') \rangle = \zeta(\theta_{\n\n'})  \label{eq:cf_def}
\ee
 where the {\em correlation function} $\zeta(\theta)$ is defined by this equation.

 Starting from the definition of the correlation function in Eq.~(\ref{eq:cf_def}),
 plug in Eq.~(\ref{eq:sht1}) and use identities in Appendix~\ref{app:curved_sky}
 to show that the correlation function is related to the power spectrum by:
\be
 \zeta(\theta) = \sum_l \frac{2l+1}{4\pi} \, C_l \, P_l(\cos\theta)  \label{eq:clcf1}
\ee
 Using orthogonality of the Legendre polynomials (Eq.~(\ref{eq:pl_orth}), show how
 to invert Eq.~(\ref{eq:clcf2}), obtaining the relation:
\be
C_l = 2\pi \int_{-1}^1 d(\cos\theta) \, \zeta(\theta) \, P_l(\cos\theta)  \label{eq:clcf2}
\ee
 What are the Euclidean analogs of Eqs.~(\ref{eq:clcf1}),~(\ref{eq:clcf2})?

\item {\bf Gaussian white noise.}
 A ``Gaussian white noise'' field $\eta(\x)$ is defined by the following statements.  Suppose that we
 measure the average value of $\eta$ over a sky pixel with angular size $d\Omega$ steradians.
 Then the result is a Gaussian random variable with mean zero and variance $\Delta^2 (d\Omega)^{-1}$,
 where $\Delta$ is a constant which parameterizes the amplitude of the noise.
 If we measure the values of $\eta$ in non-overlapping pixels, then the measurements are independent.

 First, explain why the $(d\Omega)^{-1}$ scaling makes sense.  That is, give an intuitive argument
 why the exponent should be -1, rather than some other value.

 Second, compute the correlation function $\zeta(\theta)$ and power spectrum $C_l$ of a Gaussian
 white noise field $\eta$.

\item {\bf Beam convolution.}
 Suppose that a Gaussian random field $\phi(\n)$ is convolved with an azimuthally symmetric beam profile $b(\theta)$.
 That is, we define a new random field $\psi(\n)$ by:
\be
 \psi(\n) = \int d^2\n' \, b(\theta_{\n\n'}) \, \phi(\n')
\ee
 Show that the harmonic-space representations $a_{lm}^\phi$ and $a_{lm}^\psi$ are related by:
\be
 a_{lm}^\psi = b_l a_{lm}^\phi
\ee
 where $b_l$ are the coefficients in the expansion of $b(\theta)$ in Legendre polynomials:
\be
 b(\theta) = \sum_l \frac{2l+1}{4\pi} \, b_l \, P_l(\cos\theta)
\ee
 How is the power spectrum $C_l^\psi$ related to $C_l^\phi$?

\item {\bf Fisher matrix.}
 Now consider a curved-sky Gaussian random field whose power spectrum $C_l$ depends on cosmological
 parameters $\pi_1, \cdots, \pi_N$.  We assume that we have rough fiducial guesses $\pi_i^{rm fid}$,
 and therefore a rough fiducial guess for the power spectrum $C_l^{\rm fid}$.

 Suppose that we observe the Fourier modes $a_{lm}$ over some range of multipoles $l_{\rm min} \le l \le l_{\rm max}$.
 Write an expression for the posterior likelihood function $\L[\pi_i|a_{lm}]$ of the cosmological parameters,
 given the data $a_{lm}$.

 Now recall the general definition of the Fisher matrix:
\be
 F_{ij} = -\left\langle \frac{\partial^2\log\L[\pi_i|a_{lm}]}{\partial\pi_i \partial\pi_j} \right\rangle
\ee
 where the expectation value $\langle \cdot \rangle$ is taken over random realizations of the data $a_{lm}$
 in the fiducual model.  Using your expression for the posterior likelihood, compute the Fisher matrix and
 show that it is given by:
\be
 F_{ij} = \sum_{l=l_{\rm min}}^{l_{\rm max}} \left( \frac{2l+1}{2} \right) \frac{(\partial C_l / \partial\pi_i)(\partial C_l/\partial\pi_j)}{(C_l^{\rm fid})^2}  \label{eq:F_C}
\ee
 What would be the analogous expression for the Fisher matrix for a Euclidean 2D field?

\item {\bf Noise and beam.}
 Now suppose that instead of observing the field $a_{lm}$ directly, we observe the noisy beam-convolved field $(b_l a_{lm} + \eta_{\ell m})$,
 where $b_l$ is a known beam profile (see ``Beam convolution'' above), and $\eta_{\ell m}$ is a Gaussian white noise field with known
 amplitude $\Delta$ (see ``Gaussian white noise'' above).

 In this situation, how would Eq.~(\ref{eq:F_C}) for the Fisher matrix change?
 We will use this result in the next section, to forecast parameter constraints from the CMB!

\end{enumerate}

\clearpage

\appendix

\section{Gaussian probability distribution}
\label{app:gaussian}

\par\noindent
The probability distribution of a Gaussian random variable with mean $\mu$ and variance $\sigma^2$ is
\be
p(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left( - \frac{(x-\mu)^2}{2\sigma^2} \right)
\ee
Expectation values of Gaussian random variables can be computed using Wick's theorem.
For example:
\be
\langle x \rangle = \mu
  \hspace{1cm}
\langle x^2 \rangle = \mu^2 + \sigma^2
  \hspace{1cm}
\langle x^3 \rangle = \mu^3 + 3 \sigma^2 \mu
  \hspace{1cm}
\langle x^4 \rangle = \mu^4 + 6 \sigma^2 \mu^2 + 3 \sigma^4
\ee

\section{Curved-sky special functions}
\label{app:curved_sky}

In this section, $\n = (n_x, n_y, n_z)$ denotes a unit three-vector, and a function
defined on the unit sphere will be denoted $f(\n)$.  Sometimes we will switch to 
spherical polar coordinates $(\theta,\phi)$, and write $f(\theta,\phi)$ instead,
where:
\be
n_x = (\sin\theta)(\cos\phi)
  \hspace{1cm}
n_y = (\sin\theta)(\sin\phi)
  \hspace{1cm}
n_z = \cos\theta
\ee

\subsection{Spherical harmonics}

The spherical harmonic $Y_{lm}(\n)$ is a special function defined for integers $l \ge 0$
and $-l \ge m \le l$.  Just for concreteness, the first few spherical harmonics are given by:
\ba
Y_{00}(\theta,\phi) &=& \frac{1}{\sqrt{4\pi}} \nn \\
Y_{10}(\theta,\phi) &=& \sqrt{\frac{3}{4\pi}} (\cos\theta) \nn \\
Y_{1,\pm 1}(\theta,\phi) &=& \mp \sqrt{\frac{3}{8\pi}} (\sin\theta) e^{\pm i\phi} \nn \\
Y_{20}(\theta,\phi) &=& \sqrt{\frac{5}{16\pi}} (3 \cos^2\theta - 1) \nn  \\
Y_{2,\pm 1}(\theta,\phi) &=& \mp \sqrt{\frac{15}{8\pi}} (\sin\theta) (\cos\theta) e^{\pm i \phi} \nn \\
Y_{2,\pm 2}(\theta,\phi) &=& \sqrt{\frac{15}{32\pi}} (\sin^2\theta) e^{\pm 2 i \phi}
\ea
The spherical harmonics are a {\em complete, orthonormal} basis of functions on the unit sphere.
Here, ``orthonormal'' means that:
\be
\int d^2\n \, Y_{lm}(\n) Y_{l'm'}^*(\n) = \delta_{ll'} \delta_{mm'}  \label{eq:ylm_orth}
\ee
where $d^2\n = \sin\theta \, d\theta \, d\phi$ is the rotation-invariant area element on the unit sphere.

``Complete'' means that an arbitrary function $f(\n)$ can be expanded as an infinite series:
\be
f(\n) = \sum_{l=0}^\infty \sum_{m=-l}^l a_{lm} Y_{lm}(\n)  \label{eq:ylm_completeness}
\ee
for some choice of coefficients $a_{lm}$.  In fact, we can give an explicit formula for $a_{lm}$ as follows.  
If we multiply both sides of Eq.~(\ref{eq:ylm_completeness})
by $Y_{lm}^*(\n)$ and integrate over $\n$, then a short calculation using Eq.~(\ref{eq:ylm_orth}) gives:
\be
a_{lm} = \int d^2\n \, f(\n) Y_{lm}^*(\n)
\ee
There are a lot of useful identities involving spherical harmonics, which I'll collect systematically
in a future version of these notes!  For now, one identity which will be very useful is:
\be
Y_{lm}(\n)^* = (-1)^m Y_{l,-m}(\n)
\ee

\subsection{Legendre polynomials}

The Legendre polynomial $P_l(x)$ is a special function defined for integers $l \ge 0$.
The first few Legendre polynomials are:
\be
P_0(x) = 1
  \hspace{1.5cm}
P_1(x) = x
  \hspace{1.5cm}
P_2(x) = \frac{3}{2}x^2 - \frac{1}{2}
\ee
The Legendre polynomials are an {\em orthogonal, complete} basis of functions defined on the interval $-1 \le x \le 1$.
Here, ``orthogonal'' means:
\be
\int_{-1}^1 d\mu \, P_l(\mu) P_{l'}(\mu) = \frac{2}{2l+1} \delta_{ll'}  \label{eq:pl_orth}
\ee
Note that $P_l(x)$ is not unit-normalized (that is, the integral of $P_l^2$ is not equal to 1).
Instead, the normalization is chosen so that
\be
P_l(1) = 1 \hspace{1cm} \mbox{for all $l$}
\ee
``Complete'' means that any function $f(x)$ defined for $-1 \le x \le 1$ has a series expansion:
\be
f(x) = \sum_{l=0}^\infty f_l P_l(x)  \label{eq:pl_completeness}
\ee
for some choice of coefficients $f_l$.
In fact, we can give an explicit formula for $f_l$ as follows.  
If we multiply both sides of Eq.~(\ref{eq:pl_completeness})
by $P_l(x)$ and integrate over $x$, then a short calculation using Eq.~(\ref{eq:pl_orth}) gives:
\be
f_l = \frac{2l+1}{2} \int_{-1}^1 dx \, f(x) P_l(x)
\ee
There are a lot of useful identities involving Legendre polynomials, which I'll collect systematically
in a future version of these notes!  For now, it will be very useful to have the following identity
relating spherical harmonics and Legendre polynomials:
\be
\sum_{m=-l}^l Y_{lm}^*(\n) \, Y_{lm}(\n') = \frac{2l+1}{4\pi} P_l(\n\cdot\n')
\ee

\end{document}
